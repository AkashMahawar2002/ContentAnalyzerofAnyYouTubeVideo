{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI+lqSxZA9OYZ9m81sVz8k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkashMahawar2002/ContentAnalyzerofAnyYouTubeVideo/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO8XhZlvC0Nj"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-aiplatform\n",
        "!pip install langchain\n",
        "!pip install chromadb\n",
        "!pip install pytube\n",
        "!pip install youtube-transcript-api\n",
        "!pip install gradio\n",
        "from google.cloud import aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "uCOnir2rE_9i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "PROJECT_ID = \"lab3a-405912\" #enter your project id here\n",
        "vertexai.init(project=PROJECT_ID)"
      ],
      "metadata": {
        "id": "eFOCyBZjFYRf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import YoutubeLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import VertexAI"
      ],
      "metadata": {
        "id": "dk9oqxC3F302"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = VertexAI(\n",
        "model_name=\"text-bison@001\",\n",
        "max_output_tokens=256,\n",
        "temperature=0.1,\n",
        "top_p=0.8,\n",
        "top_k=40,\n",
        "verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "clk5RXeSGGAl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "\n",
        "# Embedding\n",
        "EMBEDDING_QPM = 100\n",
        "EMBEDDING_NUM_BATCH =5\n",
        "embeddings = VertexAIEmbeddings(\n",
        "    requests_per_minute=EMBEDDING_QPM,\n",
        "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
        ")"
      ],
      "metadata": {
        "id": "iCarAAvrG0Sy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=1-izXBhkiHw\", add_video_info=True)\n",
        "result = loader.load()"
      ],
      "metadata": {
        "id": "cwyshQ4SG9K6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(result)\n",
        "print(f\"# of documents = {len(docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2LW120OHBg4",
        "outputId": "9096cca1-30be-4128-8d19-01bd55d80071"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of documents = 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db = Chroma.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "EV1FaYH_HOtZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "id": "ewekQYFpJdG1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type( llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)"
      ],
      "metadata": {
        "id": "_zIDlRmFJgqj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sm_ask(question, print_results=True):\n",
        "  video_subset = qa({\"query\": question})\n",
        "  context = video_subset\n",
        "  prompt = f\"\"\"\n",
        "  Answer the following question in a detailed manner, using information from the text below. If the answer is not in the text,say I dont know and do not generate your own response.\n",
        "\n",
        "  Question:\n",
        "  {question}\n",
        "  Text:\n",
        "  {context}\n",
        "\n",
        "  Question:\n",
        "  {question}\n",
        "\n",
        "  Answer:\n",
        "  \"\"\"\n",
        "  parameters = {\n",
        "  \"temperature\": 0.1,\n",
        "  \"max_output_tokens\": 256,\n",
        "  \"top_p\": 0.8,\n",
        "  \"top_k\": 40\n",
        "  }\n",
        "  response = llm.predict(prompt, **parameters)\n",
        "  return {\n",
        "  \"answer\": response\n",
        "\n",
        "  }"
      ],
      "metadata": {
        "id": "ieP_JGYOJufp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "def get_response(input_text):\n",
        "  response = sm_ask(input_text)\n",
        "  return response\n",
        "\n",
        "grapp = gr.Interface(fn=get_response, inputs=\"text\", outputs=\"text\")\n",
        "grapp.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "uOTucUGpJ0h2",
        "outputId": "2492b06c-7495-4107-bb87-93c9c8339a5a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://9a2fd927858982d87b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9a2fd927858982d87b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}